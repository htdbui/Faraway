{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6b8845a-44e2-4f0e-876c-fe85da25c298",
   "metadata": {},
   "source": [
    "## Lernhilfe zur Linearen Regression mit dem Galapagos-Datensatz\n",
    "\n",
    "### 1. Beschreibung der Daten\n",
    "\n",
    "Der Datensatz enthält Informationen über 30 Galapagos-Inseln.  Es gibt sechs Variablen:\n",
    "\n",
    "*   **Species:** Die Anzahl der auf der Insel gefundenen Arten.\n",
    "*   **Area:** Die Fläche der Insel (km²).\n",
    "*   **Elevation:** Die höchste Erhebung der Insel (m).\n",
    "*   **Nearest:** Die Entfernung zur nächsten Insel (km).\n",
    "*   **Scruz:** Die Entfernung zur Insel Santa Cruz (km).\n",
    "*   **Adjacent:** Die Fläche der benachbarten Insel (km²).\n",
    "\n",
    "### 2. Laden von Paketen und Daten\n",
    "\n",
    "```python\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import scipy as sp \n",
    "import statsmodels.api as sm \n",
    "import statsmodels.formula.api as smf \n",
    "import seaborn as sns \n",
    "import faraway.utils \n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import faraway.datasets.galapagos \n",
    "galapagos = faraway.datasets.galapagos.load() \n",
    "galapagos.head(3)\n",
    "print(\"Area: km², Elevation: m, Nearest: km, Scruz: km, Adjacent: km²\") \n",
    "galapagos.describe().iloc[1:,:].round(3)\n",
    "```\n",
    "\n",
    "### 3. Lineare Regression\n",
    "\n",
    "```python\n",
    "lmod = smf.ols(formula='Species ~ Area + Elevation + Nearest + Scruz  + Adjacent', data=galapagos).fit() \n",
    "lmod.summary()\n",
    "### Kürzere Version der Zusammenfassung aus dem Paket faraway\n",
    "lmod.sumary()\n",
    "```\n",
    "\n",
    "#### 3.1. Extrahieren der Regressionsgrößen\n",
    "\n",
    "##### 3.1.1. Grundlagen\n",
    "\n",
    "*   Schätzungen der Koeffizienten: `lmod.params`\n",
    "*   Beta-Standardfehler: `lmod.bse`\n",
    "*   p-Werte: `lmod.pvalues`\n",
    "*   T-Statistiken für die Koeffizienten: `lmod.tvalues`\n",
    "\n",
    "##### 3.1.2. F-Werte\n",
    "\n",
    "*   F-Statistik und ihr p-Wert: `lmod.fvalue, lmod.f_pvalue`\n",
    "\n",
    "##### 3.1.3. Konfidenzintervalle\n",
    "\n",
    "*   Konfidenzintervalle der Koeffizienten: `lmod.conf_int()`\n",
    "*   Ob die t-Verteilung für die Inferenz verwendet werden soll: `lmod.use_t`\n",
    "    *   Wahr: Die t-Verteilung wird für die Inferenz verwendet.\n",
    "    *   Falsch: Die Normalverteilung wird für die Inferenz verwendet.\n",
    "\n",
    "##### 3.1.4. Güte der Anpassung\n",
    "\n",
    "*   R-Quadrat: `lmod.rsquared`\n",
    "*   Angepasstes R-Quadrat: `lmod.rsquared_adj`\n",
    "*   Akaike Information Criterion AIC: `lmod.aic`\n",
    "*   Bayesian Information Criterion BIC: `lmod.bic`\n",
    "\n",
    "##### 3.1.5. Quadratsummen\n",
    "\n",
    "| Quelle        | Freiheitsgrade | Quadratsummen | Mittleres Quadrat     |\n",
    "| ------------- | --------------- | ------------- | --------------------- |\n",
    "| Regression    | (p-1)          | ESS           | $\\frac{\\text{ESS}}{p-1}$ |\n",
    "| Residuen      | (n-p)          | RSS           | $\\frac{\\text{RSS}}{n-p}$ |\n",
    "| Gesamt        | (n-1)          | TSS           | $\\frac{\\text{TSS}}{n-1}$ |\n",
    "\n",
    "*   Residuenquadratsumme RSS: `lmod.ssr`\n",
    "*   Quadratsumme der Residuen SSR: `lmod.ssr`\n",
    "*   Erklärte Quadratsumme ESS: `lmod.ess`\n",
    "*   Die zentrierte TSS = Summe(beobachteter Wert - Mittelwert(beobachtete Variable))²: `lmod.centered_tss, lmod.ssr + lmod.ess`\n",
    "*   Die nicht zentrierte TSS = Summe(beobachteter Wert²): `lmod.uncentered_tss, sum(lmod.model.endog**2)`\n",
    "*   Freiheitsgrade des Modells: `lmod.df_model`\n",
    "*   Freiheitsgrade der Residuen: `lmod.df_resid`\n",
    "*   Anzahl der Beobachtungen: `lmod.nobs`\n",
    "*   Mittlerer quadratischer Fehler des Modells: `lmod.mse_model, lmod.ess / lmod.df_model`\n",
    "*   Mittlerer quadratischer Fehler der Residuen: `lmod.mse_resid, lmod.scale`\n",
    "*   Gesamter mittlerer quadratischer Fehler: `lmod.mse_total, lmod.centered_tss / (lmod.df_model + lmod.df_resid)`\n",
    "\n",
    "##### 3.1.6. Angepasste Werte und Residuen\n",
    "\n",
    "*   Angepasste Werte: `lmod.fittedvalues.head(3)`\n",
    "*   Residuen: `lmod.resid.head(3)`\n",
    "*   Selbstausgerechnete Residuen: `(lmod.model.endog - lmod.fittedvalues).head(3)`\n",
    "*   Pearson-Residuen: `lmod.resid_pearson.round(3)`\n",
    "    *   Pearson-Residuen = Rohresiduen / ihre Standardabweichung: `( lmod.resid / np.sqrt(lmod.mse_resid) ).head(3)`\n",
    "\n",
    "##### 3.1.7. Kovarianzmatrix\n",
    "\n",
    "*   Kovarianzmatrix der Koeffizienten: `lmod.cov_params()`\n",
    "*   Typ der Kovarianzmatrix: `lmod.cov_type`\n",
    "*   Normalisierte Kovarianzmatrix = Kovarianzmatrix / Residuenvarianz: `lmod.normalized_cov_params`\n",
    "*   HC0 ist die grundlegende heteroskedastizitätskonsistente Kovarianzmatrix. Sie wendet keine zusätzlichen Skalierungsfaktoren an: `lmod.cov_HC0.round(3); lmod.HC0_se`\n",
    "*   HC1 wendet eine Freiheitsgradkorrektur auf die HC0-Matrix an. Sie skaliert die Residuen mit n/(n-k), wobei n die Anzahl der Beobachtungen und k die Anzahl der Parameter ist: `lmod.cov_HC1.round(3); lmod.HC1_se`\n",
    "*   HC2 passt die Residuen an, indem es durch (1 - h\\_i) dividiert, wobei h\\_i die Hebelwerte (Diagonalelemente der Hutmatrix) sind: `lmod.cov_HC2.round(3); lmod.HC2_se`\n",
    "*   HC3 passt die Residuen an, indem es durch (1 - h\\_i)² dividiert, wobei h\\_i die Hebelwerte (Diagonalelemente der Hutmatrix) sind: `lmod.cov_HC3.round(3); lmod.HC3_se`\n",
    "\n",
    "##### 3.1.8. Ausreisser-Test\n",
    "\n",
    "*   Führt einen Ausreisser-Test an dem angepassten Modell durch: `lmod.outlier_test()`\n",
    "    *   student\\_resid: Die studentisierten Residuen.\n",
    "    *   unadj\\_p: Die nicht angepassten p-Werte für den Test.\n",
    "    *   bonf(p): Die Bonferroni-angepassten p-Werte.\n",
    "\n",
    "#### 3.2. Schrittweise Berechnung der Schätzungen für Beta\n",
    "\n",
    "*   X-Matrix: `X = galapagos.iloc[:,1:]; X.head()`\n",
    "*   X'X: `X.T @ X`\n",
    "*   (X'X)^-1: `XtXi = np.linalg.inv(X.T @ X); XtXi`\n",
    "*   (X'X)^-1 X'y: `(XtXi @ X.T) @ galapagos.Species`\n",
    "*   Eine andere Möglichkeit zur Berechnung der Schätzungen mit (X'X)^-1 Beta = X'y: `np.linalg.solve(X.T @ X, X.T @ galapagos.Species)`\n",
    "\n",
    "##### 3.2.1. Verwenden der Moore-Penrose-Inversen zur Berechnung der Schätzungen\n",
    "\n",
    "*   Moore-Penrose-Inverse X^- = (X'X)^-1 X': `Xmp = np.linalg.pinv(X); Xmp.shape`\n",
    "*   Berechnen Sie die Schätzungen: `Xmp @ galapagos.Species`\n",
    "\n",
    "##### 3.2.2. Verwenden der QR-Zerlegung zur Berechnung der Schätzungen\n",
    "\n",
    "*   q, r: `q, r = np.linalg.qr(X)`\n",
    "*   f: `f = q.T @ galapagos.Species; f`\n",
    "*   Berechnen Sie die Schätzungen: `sp.linalg.solve_triangular(r, f)`\n",
    "*   Alternativ: `lmod_qr = smf.ols('Species ~ Area + Elevation + Nearest + Scruz  + Adjacent', galapagos).fit(method=\"qr\") lmod_qr.params`\n",
    "\n",
    "##### 3.2.3. Verwenden des allgemeinen Lösers für das Problem der kleinsten Quadrate\n",
    "\n",
    "*   res: Die Summe der quadrierten Residuen der Lösung. Wenn der Rang von X kleiner als die Anzahl der Spalten in X ist, ist dies ein leeres Array.\n",
    "*   rnk: Der effektive Rang der Matrix X. Dies ist die Anzahl der Singulärwerte von X, die größer als eine bestimmte Toleranz sind.\n",
    "*   s: Die Singulärwerte von X.\n",
    "\n",
    "```python\n",
    "params, res, rnk, s = sp.linalg.lstsq(X, galapagos['Species'])\n",
    "```\n",
    "\n",
    "#### 3.3. Identifizierbarkeit\n",
    "\n",
    "##### 3.3.1. Vollständige Nicht-Identifizierbarkeit\n",
    "\n",
    "```python\n",
    "galapagos['Adiff'] = galapagos.Area - galapagos.Adjacent \n",
    "lmod_ide = smf.ols('Species ~ Area+Elevation+Nearest+Scruz+Adjacent+Adiff', galapagos).fit() \n",
    "lmod_ide.sumary()\n",
    "\n",
    "### Zeige den kleinsten Eigenwert\n",
    "lmod_ide.eigenvals[-1]\n",
    "\n",
    "### Verwendung der QR-Zerlegung\n",
    "lmod_ide_qr = smf.ols('Species ~ Area+Elevation+Nearest+Scruz+Adjacent+Adiff', galapagos).fit(method=\"qr\") \n",
    "lmod_ide_qr.sumary()\n",
    "```\n",
    "\n",
    "##### 3.3.2. Experiment für nahe Nicht-Identifizierbarkeit\n",
    "\n",
    "```python\n",
    "np.random.seed(123) \n",
    "galapagos['Adiffe'] = galapagos.Adiff + (np.random.rand(30)-0.5)*0.001 \n",
    "lmod_ide_ex = smf.ols('Species ~ Area+Elevation+Nearest+Scruz+Adjacent+Adiffe', galapagos).fit() \n",
    "lmod_ide_ex.sumary()\n",
    "\n",
    "lmod_ide_ex_qr = smf.ols('Species ~ Area+Elevation+Nearest+Scruz+Adjacent+Adiffe', galapagos).fit(method=\"qr\") \n",
    "lmod_ide_ex_qr.sumary()\n",
    "```\n",
    "\n",
    "#### 3.4. Erklärung\n",
    "\n",
    "Um die Wirkung der Erhebung zu verstehen, können wir das **vollständige Modell** (`lmod`) mit einem **reduzierten Modell** (`lmodr`), das nur die Erhebung als Prädiktor enthält, vergleichen.\n",
    "\n",
    "*   Ein **Effektplot** kann verwendet werden, um die Bedeutung des Modells für einen bestimmten Prädiktor, in diesem Fall die Erhebung, zu verstehen. Der Plot zeigt die vorhergesagte Anzahl von Arten in Abhängigkeit von der Erhebung für beide Modelle.\n",
    "\n",
    "**Wichtige Punkte:**\n",
    "\n",
    "*   Das Konzept, Variablen konstant zu halten, ist im Kontext der Galapagos-Daten nicht sinnvoll, da es sich um Beobachtungsdaten handelt.\n",
    "*   Wir behaupten **keine Kausalität** in unserer Erklärung.\n",
    "*   Vergleiche zwischen Modellen können uns Einblicke geben, aber die Informationen sind **nicht absolut** und können sich ändern.\n",
    "\n",
    "### 4. Hypothesentests\n",
    "\n",
    "In der linearen Regression verwenden wir Hypothesentests, um die Signifikanz der Prädiktoren im Modell zu beurteilen. Die Quellen beschreiben verschiedene Arten von Hypothesentests, die in diesem Kontext durchgeführt werden können.\n",
    "\n",
    "#### 4.1. Test aller Prädiktoren\n",
    "\n",
    "Dieser Test prüft die Hypothese, dass **alle** Prädiktoren im Modell keinen Einfluss auf die Antwortvariable haben.\n",
    "\n",
    "*   **Nullhypothese:** $H_0: \\beta_1 = \\cdots = \\beta_{p-1}=0$\n",
    "*   **Alternativhypothese:** Mindestens ein $\\beta_j$ ist ungleich Null.\n",
    "\n",
    "Die **F-Statistik** wird verwendet, um diese Hypothese zu testen. Sie vergleicht die Anpassung des vollständigen Modells (mit allen Prädiktoren) mit der Anpassung eines reduzierten Modells (nur mit dem Intercept). \n",
    "\n",
    "**Schritt-für-Schritt Berechnung der F-Statistik und des p-Werts:**\n",
    "\n",
    "1.  **RSS des reduzierten Modells:** Dies ist die gesamte Quadratsumme (TSS), die in mit `lmod.centered_tss` berechnet werden kann.\n",
    "2.  **RSS des vollständigen Modells:** `lmod.ssr`\n",
    "3.  **Freiheitsgrade des vollständigen Modells:** `lmod.df_resid`\n",
    "4.  **F-Statistik:**  `lmod.mse_model / lmod.mse_resid`. Dies entspricht der Formel $\\frac{(TSS-RSS)/(p-1)}{RSS/(n-p)}$.\n",
    "5.  **p-Wert der F-Statistik:** `1-sp.stats.f.cdf(lmod.fvalue, lmod.df_model, lmod.df_resid)`.\n",
    "\n",
    "Die Quellen zeigen auch, wie man diesen Test mit `lmod.fvalue`, `lmod.f_pvalue` oder `lmod.compare_f_test(lmodr)` durchführen kann.\n",
    "\n",
    "#### 4.2. Testen eines Prädiktors\n",
    "\n",
    "Dieser Test bewertet die Signifikanz eines **einzelnen** Prädiktors im Modell. Es wird geprüft, ob das Entfernen dieses Prädiktors die Modellanpassung signifikant verschlechtert. \n",
    "\n",
    "*   Die F-Statistik kann auch hier verwendet werden, wobei das reduzierte Modell alle Prädiktoren außer dem zu testenden enthält. \n",
    "*   Alternativ kann die **t-Statistik** verwendet werden. Diese bewertet, wie viele Standardfehler der geschätzte Koeffizient von Null entfernt ist. \n",
    "*   **Wichtig:** Die Quellen weisen darauf hin, dass man sich nicht nur auf die p-Werte verlassen sollte, um die praktische Bedeutung eines Prädiktors zu bestimmen. Ein kleiner p-Wert bedeutet lediglich statistische Signifikanz, nicht unbedingt praktische Relevanz.\n",
    "\n",
    "#### 4.3. Test eines Paares von Prädiktoren\n",
    "\n",
    "Analog zu 4.2 kann man auch die Signifikanz von **zwei** Prädiktoren gleichzeitig testen. Die Quellen erwähnen jedoch, dass die Interpretation der p-Werte der einzelnen t-Tests in diesem Fall problematisch ist. Es wird empfohlen, einen einzigen F-Test zu verwenden, um mehrere Prädiktoren gleichzeitig zu testen.\n",
    "\n",
    "#### 4.4. Test eines Unterraums\n",
    "\n",
    "Man kann auch Hypothesen über **lineare Kombinationen** von Prädiktoren testen. Die Quellen geben Beispiele wie $H_0: β_{Area}=β_{Adjacent}$ und $H_0: β_{Elevation}=0.5$. Diese Tests werden ebenfalls mit der F-Statistik durchgeführt.\n",
    "\n",
    "#### 4.5. Einschränkungen des Tests\n",
    "\n",
    "Die F-Tests sind nicht universell einsetzbar. \n",
    "\n",
    "*   Sie können keine nichtlinearen Hypothesen testen. \n",
    "*   Sie können keine Modelle vergleichen, die nicht verschachtelt sind oder unterschiedliche Prädiktoren haben. \n",
    "*   Sie sind nicht direkt anwendbar, wenn die Modelle unterschiedliche Datensätze verwenden oder fehlende Werte haben.\n",
    "\n",
    "#### 4.6. Permutationstests\n",
    "\n",
    "Permutationstests sind eine **Alternative** zu den F-Tests, die **keine Normalverteilungsannahme** für die Fehler benötigen. Sie basieren auf der Idee, dass die beobachteten Daten zufällig permutiert werden, wenn die Nullhypothese (kein Zusammenhang zwischen Prädiktoren und Antwort) zutrifft.  Die Quellen beschreiben detailliert, wie Permutationstests für den Test aller Prädiktoren und für den Test eines Prädiktors durchgeführt werden können.\n",
    "\n",
    "### 5. Konfidenzintervalle für $\\beta$\n",
    "\n",
    "Konfidenzintervalle geben einen Bereich an, in dem der wahre Wert eines Parameters (hier: die Regressionskoeffizienten $\\beta$) mit einer bestimmten Wahrscheinlichkeit liegt.\n",
    "\n",
    "*   **Berechnung:** $\\hat{β} *i \\pm t* {\\alpha/2, t-n} se(\\hat{β})$\n",
    "    *   $\\hat{β} *i$: Geschätzter Koeffizient\n",
    "    *   $t* {\\alpha/2, t-n}$: Kritischer Wert der t-Verteilung\n",
    "    *   $se(\\hat{β})$: Standardfehler des Koeffizienten\n",
    "*   **Interpretation:** Wenn wir ein 95%-Konfidenzintervall berechnen, bedeutet dies, dass wir zu 95% sicher sind, dass der wahre Wert des Parameters innerhalb dieses Intervalls liegt.\n",
    "*   **Zusammenhang mit Hypothesentests:** Die Quellen weisen darauf hin, dass wir, wenn wir ein Konfidenzintervall von (1-α)% wählen, nur Tests auf dem Signifikanzniveau von α% durchführen können.\n",
    "*   **Vorteile von Konfidenzintervallen:** Sie geben uns mehr Informationen über die Größe des Effekts eines Prädiktors und sind daher informativer als reine p-Werte.\n",
    "\n",
    "**Beispiel im Code:**\n",
    "\n",
    "*   Kritische Werte der t-Verteilung: `qt = np.array(sp.stats.t.interval(0.95,24))`\n",
    "*   Berechnung des Konfidenzintervalls: `lmod.params[\"Area\"] + lmod.bse[\"Area\"]*qt`\n",
    "*   Alle Konfidenzintervalle: `lmod.conf_int()`\n",
    "\n",
    "#### Bootstrap-Konfidenzintervalle\n",
    "\n",
    "Bootstrap-Methoden sind eine **nichtparametrische Methode**, um Konfidenzintervalle zu konstruieren, die **keine Normalverteilungsannahme** erfordern. \n",
    "\n",
    "*   **Grundprinzip:** Anstatt die wahre Verteilung der Daten zu kennen, wird wiederholt aus den beobachteten Daten resampelt (mit Zurücklegen). \n",
    "*   **Vorteil:** Die Methode kann auch dann angewendet werden, wenn theoretische Berechnungen schwierig oder unmöglich sind.\n",
    "*   **Anwendung:** Die Quellen beschreiben detailliert, wie Bootstrap-Konfidenzintervalle für die Regressionskoeffizienten berechnet werden können.\n",
    "\n",
    "### 6. Diagnose\n",
    "\n",
    "Nach der Anpassung eines Regressionsmodells ist es wichtig, die **Modellannahmen zu überprüfen**. Die Quellen konzentrieren sich hier auf die Annahme der **konstanten Varianz** der Fehler.\n",
    "\n",
    "#### 6.1. Konstante Varianz\n",
    "\n",
    "*   **Überprüfung:** Man kann die Residuen gegen die angepassten Werte plotten. Wenn die Varianz konstant ist, sollten die Punkte zufällig um die Nulllinie streuen.\n",
    "*   **Beispiel im Code:** `plt.scatter(lmod.fittedvalues, lmod.resid)`\n",
    "*   **Transformation:** Wenn die Varianz nicht konstant ist, kann eine Transformation der Antwortvariable hilfreich sein. Die Quellen geben das Beispiel einer Quadratwurzeltransformation für Zähldaten.\n",
    "\n",
    "### 7. Robuste Regression\n",
    "\n",
    "Robuste Regressionsmethoden sind weniger empfindlich gegenüber **Ausreißern** als die Methode der kleinsten Quadrate.\n",
    "\n",
    "*   **M-Schätzung:** Ein allgemeiner Ansatz für robuste Regression, bei dem die Koeffizienten so gewählt werden, dass eine bestimmte Funktion der Residuen minimiert wird.\n",
    "*   **Gewichtete kleinste Quadrate:** M-Schätzung kann als eine Form der gewichteten kleinsten Quadrate interpretiert werden, wobei die Gewichte von den Residuen abhängen.\n",
    "*   **Vergleich mit OLS:** Die Quellen vergleichen die Ergebnisse von OLS mit robusten Regressionsmethoden und stellen fest, dass sich die Koeffizientenwerte leicht verschieben und die Standardfehler im Allgemeinen reduziert werden.\n",
    "*   **Gewichte:** Die Analyse der Gewichte aus der robusten Anpassung kann Aufschluss darüber geben, welche Beobachtungen die Anpassung am stärksten beeinflussen.\n",
    "*   **Einschränkungen:** Robuste Regression ist kein Allheilmittel. Sie löst nicht das Problem von Punkten mit großem Einfluss und hilft nicht bei der Auswahl von Prädiktoren oder der Transformation von Variablen.\n",
    "\n",
    "### 8. Transformation\n",
    "\n",
    "Manchmal kann eine **Transformation der Antwortvariable** die Modellanpassung verbessern und die Interpretation erleichtern. \n",
    "\n",
    "*   **Log-Transformation:** Bei einer Log-Transformation der Antwortvariable haben die Regressionskoeffizienten eine multiplikative Interpretation.\n",
    "*   **Box-Cox-Methode:** Eine allgemeine Methode, um die optimale Transformation für die Antwortvariable zu finden.\n",
    "*   **Auswahl von λ:** Der optimale Wert von λ kann durch Maximierung der Profil-Log-Likelihood bestimmt werden.\n",
    "*   **Konfidenzintervall für λ:** Ein Konfidenzintervall für λ kann verwendet werden, um zu beurteilen, wie viel Rundung von λ für die Interpretierbarkeit sinnvoll ist.\n",
    "\n",
    "**Beispiel im Code:**\n",
    "\n",
    "Die Quellen zeigen, wie die Box-Cox-Methode mit Python implementiert werden kann, um die optimale Transformation für die Galapagos-Daten zu finden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
