# 1. Describe the data
- It has 3236 rows.
- There are 9 varialbes:
    - school: A code representing 50 different schools (1-50).
    - class: A factor indicating the class level (1, 2, 3, 4).
    - gender: A factor with levels "boy" and "girl".
    - social: The social class of the father, coded as:
      - I = 1
      - II = 2
      - III nonmanual = 3
      - III manual = 4
      - IV = 5
      - V = 6
      - Long-term unemployed = 7
      - Not currently employed = 8
      - Father absent = 9
    - raven: The test score from the Raven's Progressive Matrices.
    - id: A unique student ID (coded 1-1402).
    - english: The score in English.
    - math: The score in Maths.
    - year: The year of school.
- Consider data from the Junior School Project collected from primary schools in inner London, detailed in Mortimore et al. (1988). We focus on two variables: the school (49 in total) and the mathematics test scores of their students.

# 2. Load packages and data

# 3. False Discovery Rate

- Since the response is centered, t-tests for differences from zero are meaningful. Schools 1 and 50 are significantly below average, while school 2 is not significantly above average. We can test for differences between schools:

- We find a strongly significant difference, unsurprising given the large sample size and expected differences between schools. The more interesting question is which schools show clear evidence of under- or over-performance?
- There are too many pairwise comparisons to focus on. Instead, let's identify schools with means significantly different from the average. Our parameterization simplifies these comparisons, but we expect about 5% of differences to be significant under the null hypothesis.
- Some adjustment is necessary. One approach is to control the familywise error rate (FWER), the overall probability of falsely declaring a difference. The Bonferroni correction is a simple method: multiply the unadjusted p-values by the number of comparisons, truncating any value above one. Let's see which schools have adjusted p-values less than 5%.
- The familywise error rate (FWER) is defined as the probability of making one or more Type I errors among a family of hypotheses tests. Mathematically: $ \text{FWER} = P(\text{At least one Type I error}) $. When conducting multiple comparisons, controlling the FWER ensures that the probability of at least one false positive remains below a specified significance level, typically denoted as $alpha$.

- Eight schools are identified, with all except school 31 significantly above average.
- The Bonferroni correction is conservative, and even more lenient methods impose stringent requirements as the number of comparisons increases.
- An alternative is to control the false discovery rate (FDR), the proportion of falsely identified significant effects, using the Benjamini and Hochberg (1995) method.
    - In this method, given sorted p-values $ p_{(i)} $ for $ i = 1, \ldots, m $, the procedure finds the largest index $ i $ where $ p_{(i)} \leq \alpha \frac{i}{m} $. All tests corresponding to p-values up to and including this index are considered significant.

- Eighteen schools are identified compared to eight by the previous procedure.
- FDR is less stringent than FWER in identifying significant effects.
- A more convenient method of computing the adjusted p-values is:

- FDR methods are common in imaging and bioinformatics, where large numbers of comparisons are necessary. They are useful for reliably identifying significant effects in such applications.